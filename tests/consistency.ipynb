{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28dea2b5",
   "metadata": {},
   "source": [
    "# Тест на стабильность и регрессию - Save/Load Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1562e06",
   "metadata": {},
   "source": [
    "## Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "995340fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Библиотеки ипортированы:\n",
      "tensorflow 2.19.0\n",
      "numpy 2.1.3\n",
      "pandas 2.3.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "\n",
    "print(f'Библиотеки ипортированы:')\n",
    "print('tensorflow', tf.__version__)\n",
    "print('numpy', np.__version__)\n",
    "print('pandas', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea602eb",
   "metadata": {},
   "source": [
    "## Настройки логирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c6d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format=\"%(message)s\")\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc890c31",
   "metadata": {},
   "source": [
    "## Конфигурация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0771dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"../model/model.keras\"\n",
    "TOKENIZER_PATH = \"../model/tokenizer/tokenizer.json\"\n",
    "CSV_PATH = \"../data/processed/dataset.csv\"\n",
    "MAX_LEN = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a3521b",
   "metadata": {},
   "source": [
    "## Проверка конфигурации на наличие файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e067011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель импортирована из пути: ../model/model.keras\n",
      "Токенизатор импортирован из пути: ../model/tokenizer/tokenizer.json\n",
      "Датасет импортирован из пути: ../data/processed/dataset.csv\n",
      "Конфигурация гиперпараметра MAX_LEN установлена: 25\n"
     ]
    }
   ],
   "source": [
    "# Проверка наличия каждого файла\n",
    "missing_files = []\n",
    "\n",
    "if not os.path.isfile(MODEL_PATH):\n",
    "    missing_files.append(MODEL_PATH)\n",
    "if not os.path.isfile(TOKENIZER_PATH):\n",
    "    missing_files.append(TOKENIZER_PATH)\n",
    "if not os.path.isfile(CSV_PATH):\n",
    "    missing_files.append(CSV_PATH)\n",
    "\n",
    "# Вывод результата\n",
    "if missing_files:\n",
    "    print(\"Ошибка! Не найдены следующие файлы:\")\n",
    "    for file in missing_files:\n",
    "        print(f\"   - {file}\")\n",
    "else:\n",
    "    print(f\"Модель импортирована из пути: {MODEL_PATH}\")\n",
    "    print(f\"Токенизатор импортирован из пути: {TOKENIZER_PATH}\")\n",
    "    print(f\"Датасет импортирован из пути: {CSV_PATH}\")\n",
    "    print(f\"Конфигурация гиперпараметра MAX_LEN установлена: {MAX_LEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea33c727",
   "metadata": {},
   "source": [
    "## Загрузка модели, токенайзера и датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7009eb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл модели найден: ../model/model.keras\n",
      "Модель успешно загружена.\n"
     ]
    }
   ],
   "source": [
    "# Загрузка модели\n",
    "# Проверка существования файла перед загрузкой\n",
    "if os.path.isfile(MODEL_PATH):\n",
    "    print(f\"Файл модели найден: {MODEL_PATH}\")\n",
    "    model = load_model(MODEL_PATH)\n",
    "    print(\"Модель успешно загружена.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"айл модели не найден по пути: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d6b81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл токенизатора найден: ../model/tokenizer/tokenizer.json\n",
      "Токенизатор успешно загружен.\n"
     ]
    }
   ],
   "source": [
    "# Загрузка токенизатора\n",
    "# Проверка существования файла перед загрузкой токенизатора\n",
    "if os.path.isfile(TOKENIZER_PATH):\n",
    "    print(f\"Файл токенизатора найден: {TOKENIZER_PATH}\")\n",
    "    with open(TOKENIZER_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        tokenizer = tokenizer_from_json(f.read())\n",
    "    print(\"Токенизатор успешно загружен.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Файл токенизатора не найден по пути: {TOKENIZER_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c457098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Файл найден: ../data/processed/dataset.csv\n",
      "Примеры 'грязных' слов: ['Apenines', 'Appenines', 'Athenean', 'Atheneans', 'Bernouilli', 'Blitzkreig', 'Brasillian', 'Britian', 'Brittish', 'Ceasar']\n"
     ]
    }
   ],
   "source": [
    "# Загрузка датасета\n",
    "# Проверка существования файла\n",
    "if not os.path.isfile(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"Файл не найден по пути: {CSV_PATH}\")\n",
    "print(f\"Файл найден: {CSV_PATH}\")\n",
    "\n",
    "# Загрузка датасета\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Проверка наличия колонки \"noisy\"\n",
    "if \"noisy\" not in df.columns:\n",
    "    raise ValueError(\"В датасете отсутствует колонка 'noisy'.\")\n",
    "\n",
    "# Получение первых 10 'грязных' слов\n",
    "sample_texts = df[\"noisy\"].astype(str).head(10).tolist()\n",
    "print(\"Примеры 'грязных' слов:\", sample_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c49b46",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "789d8f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Примеры входных слов: ['Apenines', 'Appenines', 'Athenean', 'Atheneans', 'Bernouilli', 'Blitzkreig', 'Brasillian', 'Britian', 'Brittish', 'Ceasar']\n",
      "Токенизация и паддинг входных данных...\n"
     ]
    }
   ],
   "source": [
    "sample_texts = df[\"noisy\"].astype(str).head(10).tolist()\n",
    "logger.info(f\"Примеры входных слов: {sample_texts}\")\n",
    "\n",
    "logger.info(\"Токенизация и паддинг входных данных...\")\n",
    "sequences = tokenizer.texts_to_sequences(sample_texts)\n",
    "padded = pad_sequences(sequences, maxlen=MAX_LEN, padding=\"post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efb2e9",
   "metadata": {},
   "source": [
    "## Функция проверки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3cf4630a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_save_load_consistency(model, sample_data, path, atol=1e-6):\n",
    "    logger.info(\"Предсказания от исходной модели...\")\n",
    "    preds1 = model.predict(sample_data)\n",
    "\n",
    "    logger.info(f\"Сохраняем модель во временный файл: {path}\")\n",
    "    model.save(path)\n",
    "\n",
    "    logger.info(\"Загружаем модель из файла...\")\n",
    "    loaded_model = keras.models.load_model(path)\n",
    "\n",
    "    logger.info(\"Предсказания от загруженной модели...\")\n",
    "    preds2 = loaded_model.predict(sample_data)\n",
    "\n",
    "    logger.info(f\"Сравниваем предсказания (atol={atol})...\")\n",
    "    consistent = np.allclose(preds1, preds2, atol=atol)\n",
    "\n",
    "    if consistent:\n",
    "        logger.info(\"\\n✅ Проверка пройдена: модель стабильно сохраняется.\")\n",
    "        logger.info(\"✅ Предсказания до и после одинаковые.\")\n",
    "    else:\n",
    "        logger.warning(\"\\n❌ Проверка провалена!\")\n",
    "        logger.warning(\"❌ Предсказания отличаются! Проверь структуру или веса.\\n\")\n",
    "\n",
    "    return consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64c7666",
   "metadata": {},
   "source": [
    "## Использование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fff8fb2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Запуск проверки стабильности модели...\n",
      "Предсказания от исходной модели...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 685ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Сохраняем модель во временный файл: C:\\Users\\Rreeo\\AppData\\Local\\Temp\\tmpuwwzzrpi.keras\n",
      "Загружаем модель из файла...\n",
      "Предсказания от загруженной модели...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 649ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Сравниваем предсказания (atol=1e-06)...\n",
      "\n",
      "✅ Проверка пройдена: модель стабильно сохраняется.\n",
      "✅ Предсказания до и после одинаковые.\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"Запуск проверки стабильности модели...\")\n",
    "tmp_file = tempfile.NamedTemporaryFile(suffix=\".keras\", delete=False).name\n",
    "ok = check_save_load_consistency(model, padded, tmp_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
